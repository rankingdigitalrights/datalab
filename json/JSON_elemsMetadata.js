var elemsMetadata = {
  "index": "RDR2020",
  "lastRevised": "2020-07-13",
  "meta": [{

    "indicator": "G1",
    "label": "G1. Policy Commitment",
    "description": "The company should publish a formal policy commitment to respect users’ human rights to freedom of expression and information and privacy.",
    "description_terms": ["policy commitment"],
    "elements": ["Does the company make an explicit, clearly articulated policy commitment to human rights, including to freedom of expression and information?", "Does the company make an explicit, clearly articulated policy commitment to human rights, including to privacy?", "Does the company disclose an explicit, clearly articulated policy commitment to human rights in its development and use of algorithmic systems?"],
    "elements_tags": ["explicit", "policy commitment"],
    "guidance": "This indicator seeks evidence that the company has made explicit policy commitments to freedom of expression and information, and to privacy. These standards are outlined in the U.N. Guiding Principles on Business and Human Rights’ Operational Principle 16, which states that companies should adopt formal policies publicly affirming their commitments to international human rights principles and standards. Companies should also publish a formal commitment to uphold human rights as they develop and deploy algorithmic decision making systems, in line with Council of Europe recommendations, in its Recommendation on the human rights impacts of algorithmic systems (2020). The company should clearly disclose these commitments in formal policy documents or other communications that reflect official company policy. Potential sources: Company human rights policy Company statements, reports, or other communications that reflect official company policy Company annual report or sustainability report Company “AI principles” policy"
  }, {
    "indicator": "G2",
    "label": "G2. Governance and management oversight",
    "description": "The company’s senior leadership should exercise oversight over how its policies and practices affect freedom of expression and information, and privacy.",
    "description_terms": ["senior leadership", "oversight"],
    "elements": ["Does the company clearly disclose that the board of directors exercises formal oversight over how company practices affect freedom of expression and information?", "Does the company clearly disclose that the board of directors exercises formal oversight over how company practices affect privacy?", "Does the company clearly disclose that an executive-level committee, team, program or officer oversees how company practices affect freedom of expression and information?", "Does the company clearly disclose that an executive-level committee, team, program or officer oversees how company practices affect privacy?", "Does the company clearly disclose that a clearly disclose committee, team, program or officer oversees how company practices affect freedom of expression and information?", "Does the company clearly disclose that a clearly disclose committee, team, program or officer oversees how company practices affect privacy?"],
    "elements_tags": ["clearly disclose", "board of directors", "oversight", "executive-level", "oversees"],
    "guidance": "This indicator seeks evidence that the company has strong governance and oversight over freedom of expression and information and privacy issues at all levels of its operations. Companies should clearly disclose that senior leadership—from the board to management level—oversees and is accountable for its policies and practices affecting these human rights. To receive full credit for this indicator, companies need to clearly disclose that at each governance level (board, executive, managerial), there is clear oversight of both freedom of expression and information, and privacy issues. At the board level, this oversight could include a board of directors or another public explanation of how the board exercises oversight over these issues. Below board level, it can include a company unit, program, or individual that reports to the executive or managerial level. The committee, program, team, officer, etc. should specifically identify freedom of expression and privacy in its description of responsibilities. Potential sources: List of board of directors Company governance documents Company sustainability report Company organizational chart Company human rights policy Global Network Initiative documents (if company is a member)"
  }, {
    "indicator": "G3",
    "label": "G3. Internal implementation",
    "description": "The company should have mechanisms in place to implement its commitments to freedom of expression and information and privacy within the company.",
    "description_terms": [],
    "elements": ["Does the company clearly disclose that it provides employee training on freedom of expression and information issues?", "Does the company clearly disclose that it provides employee training on privacy issues?", "Does the company clearly disclose that it maintains an employee whistleblower program through which employees can report concerns related to how the company treats its users’ freedom of expression and information rights?", "Does the company clearly disclose that it maintains an employee whistleblower program through which employees can report concerns related to how the company treats its users’ privacy rights?"],
    "elements_tags": ["clearly disclose", "whistleblower program"],
    "guidance": "Indicator G2 evaluates whether a company’s senior leadership commits to overseeing freedom of expression and privacy issues. This indicator, G3, evaluates if the company discloses whether and how these commitments are institutionalized across the company. More specifically, this indicator seeks disclosure of whether and how the company helps employees understand the importance of freedom of expression and privacy. When employees write computer code for a new product, review requests for user data, or answer customer questions about how to use a service, they act in ways that can directly affect users’ freedom of expression and privacy. We expect companies to disclose information about whether they provide training that informs employees of their role in respecting human rights and that provides employees with an outlet to voice concerns they have regarding human rights. A company can only receive full credit on this indicator if it clearly discloses information about employee training on freedom of expression and information, and privacy, as well as the existence of whistleblower programs addressing these issues. Disclosure should specify that employee training and whistleblower programs cover freedom of expression and privacy. Companies may still receive credit on this indicator if a company’s whistleblower program does not specifically mention complaints related to freedom of expression and privacy so long as the company has made commitments to these principles elsewhere and in a way that makes clear that the company would entertain those complaints through their whistleblower program. Potential sources: Company code of conduct Employee handbook Company organizational chart Company CSR/sustainability report Company blog posts"
  }, {
    "indicator": "G4a",
    "label": "G4(a). Impact assessment: Governments and regulations",
    "description": "Companies should conduct regular, comprehensive, and credible due diligence, through robust  human rights impact assessments, to identify how government regulations and policies affect freedom of expression and information and privacy, and to mitigate any risks posed by those impacts in the jurisdictions in which it operates.",
    "description_terms": ["human rights impact assessments"],
    "elements": ["Does the company  assess how laws affect freedom of expression and information in jurisdictions where it operates?", "Does the company  assess how laws affect privacy in jurisdictions where it operates?", "Does the company  assess freedom of expression and information risks associated with existing products and services in jurisdictions where it operates?", "Does the company  assess privacy risks associated with existing products and services in jurisdictions where it operates?", "Does the company  assess freedom of expression and information risks associated with a new activity, including the launch and/or acquisition of new products, services, or companies, or entry into new markets or jurisdictions?", "Does the company  assess privacy risks associated with a new activity, including the launch and/or acquisition of new products, services, or companies, or entry into new markets or jurisdictions?", "Does the company conduct additional evaluation whenever the company’s  risk assessments identify concerns?", "Do senior executives and/or members of the company’s board of directors review and consider the results of assessments and due diligence in their decision-making?", "Does the company conduct assessments on a regular schedule?", "Are the company’s assessments assured by an external third party?"],
    "elements_tags": ["assess", "risk assessments", "senior executives", "board of directors", "assessments", "third party"],
    "guidance": "This indicator examines whether companies conduct regular, robust, and accountable human rights risk assessments of government regulations and policies in the jurisdictions in which they operate. These assessments should be part of the company’s formal, systematic due diligence activities that are aimed at ensuring that their decisions and practices do not cause, contribute to, or exacerbate human rights harms. Assessments enable companies to identify possible risks to users’ freedom of expression and privacy rights and to take steps to mitigate possible harms if they are identified. Note that this indicator does not expect companies to publish detailed results of their human rights impact assessments, since assessments may include sensitive information. Rather, it expects that companies should disclose that they conduct HRIAs and provide information on what their HRIA process encompasses. Potential sources: Company CSR/sustainability reports Company human rights policy Global Network Initiative assessment reports"
  }, {
    "indicator": "G4b",
    "label": "G4(b). Impact assessment: Processes for policy enforcement",
    "description": "The company should conduct regular, comprehensive, and credible due diligence, such as through robust  human rights impact assessments, to identify how its processes for policy enforcement affect users’ fundamental rights to freedom of expression and information, to privacy, and to non-discrimination, and to mitigate any risks posed by those impacts.",
    "description_terms": ["human rights impact assessments"],
    "elements": ["Does the company  assess freedom of expression and information risks of enforcing its terms of service?", "Does the company conduct  risk assessments of its enforcement of its privacy policies?", "Does the company  assess discrimination risks associated with its processes for enforcing its  terms of service?", "Does the company  assess discrimination risks associated with its processes for enforcing its  privacy policies?", "Does the company conduct additional evaluation whenever the company’s  risk assessments identify concerns?", "Do senior executives and/or members of the company’s board of directors review and consider the results of assessments and due diligence in their decision-making?", "Does the company conduct assessments on a regular schedule?", "Are the company’s assessments assured by an external third party?", "Is the external third party that assures the assessments accredited to a relevant and reputable human rights standard by a credible organization?"],
    "elements_tags": ["assess", "risk assessments", "terms of service", "privacy policies", "senior executives", "board of directors", "assessments", "third party"],
    "guidance": "This indicator examines whether companies disclose if they conduct robust, regular, and accountable human rights risk assessments of the impact of their own policies on users’ fundamental rights to freedom of expression, privacy, and non-discrimination. These assessments should be part of the company’s formal, systematic due diligence activities that are aimed at ensuring that a company’s decisions and practices do not cause, contribute to, or exacerbate human rights harms. Assessments enable companies to identify possible risks of their own policies to users’ rights to expression and information, privacy, and to non-discrimination, and to take steps to mitigate possible harms if they are identified. Note that this indicator does not expect companies to publish detailed results of their human rights impact assessments, since assessments may include sensitive information. Rather, it expects that companies should disclose that they conduct HRIAs and provide information on what their HRIA process encompasses. Potential sources: Company CSR/sustainability reports Company human rights policy Global Network Initiative assessment reports"
  }, {
    "indicator": "G4c",
    "label": "G4(c) Impact assessment: Targeted advertising",
    "description": "The company should conduct regular, comprehensive, and credible due diligence, such as through robust  human rights impact assessments, to identify how all aspects of its targeted advertising policies and practices affect users’ fundamental rights to freedom of expression and information, to privacy, and to non-discrimination, and to mitigate any risks posed by those impacts.",
    "description_terms": ["human rights impact assessments"],
    "elements": ["Does the company  assess freedom of expression and information risks associated with its  targeted advertising policies and practices?", "Does the company  assess privacy risks associated with its  targeted advertising policies and practices?", "Does the company  assess discrimination risks associated with its targeted advertising policies and practices?", "Does the company conduct additional evaluation whenever the company’s  risk assessments identify concerns?", "Do senior executives and/or members of the company’s board of directors review and consider the results of assessments and due diligence in their decision-making?", "Does the company conduct a assessments on a regular schedule?", "Are the company’s assessments assured by an external third party?", "Is the external third party that assures the assessment accredited to a relevant and reputable human rights standard by a credible organization?"],
    "elements_tags": ["assess", "targeted advertising", "risk assessments", "senior executives", "board of directors", "assessments", "third party", "assessment"],
    "guidance": "Targeted advertising can have adverse affects on human rights, specifically on users’ rights to freedom of information, freedom of information, and freedom from discrimination. Discrimination occurs when platforms allow third-party advertisers to show different advertisements to different users on the basis of disclosed and inferred information, including membership in protected categories (race, ethnicity, age, gender identity and expression, sexual orientation, health, disability, etc.). Discrimination need not be illegal or immediately harmful to result in harmful effects at scale, such as at the population level or over the course of an individual’s lifetime. Considering the fact that targeted advertisements are less transparent than other forms of advertisement and companies’ significant financial incentives to deploy the technology quickly, these potential rights harms need to be considered in risk assessments. This indicator examines whether companies disclose if they conduct robust, regular, and accountable human rights risk assessments of the impact of targeted advertising on users’ fundamental rights to freedom of expression and information, privacy, and non-discrimination. These assessments should be part of the company’s formal, systematic due diligence activities that are aimed at ensuring that a company’s decisions and practices do not cause, contribute to, or exacerbate human rights harms. Assessments enable companies to identify possible risks of targeted advertising policies and practices on users’ human rights and to take steps to mitigate possible harms if they are identified. Note that this indicator does not expect companies to publish detailed results of their human rights impact assessments, since assessments may include sensitive information. Rather, it expects that companies should disclose that they conduct HRIAs and provide information on what their HRIA process encompasses. Potential sources: Company CSR/sustainability reports Company human rights policy Global Network Initiative assessment reports"
  }, {
    "indicator": "G4d",
    "label": "G4(d). Impact assessment: Algorithmic systems",
    "description": "The company should conduct regular, comprehensive, and credible due diligence, such as through robust human rights impact assessments, to identify how all aspects of its policies and practices related to the development and use of algorithmic systems affect users’ fundamental rights to freedom of expression and information, to privacy, and to non-discrimination, and to mitigate any risks posed by those impacts.",
    "description_terms": ["human rights impact assessments", "algorithmic systems", "non-discrimination"],
    "elements": ["Does the company assess freedom of expression and information risks associated with its development and use of algorithmic systems?", "Does the company assess privacy risks associated with its development and use of algorithmic systems?", "Does the company assess discrimination risks associated with its development and use of algorithmic systems?", "Does the company conduct additional evaluation whenever the company’s risk assessments identify concerns?", "Do senior executives and/or members of the company’s board of directors review and consider the results of assessments and due diligence in their decision-making?", "Does the company conduct assessments on a regular schedule?", "Are the company’s assessments assured by an external third party?", "Is the external third party that assures the assessment accredited to a relevant and reputable human rights standard by a credible organization?"],
    "elements_tags": ["assess", "algorithmic systems?", "algorithmic systems", "risk assessments", "senior executives", "board of directors", "assessments", "third party", "assessment"],
    "guidance": "There are a variety of ways in which algorithmic systems may pose harms to human rights. The development of such systems can rely on user information, often without the knowledge or explicit, informed consent of the data subject, constituting a privacy violation. Such systems can also cause or contribute to expression and information harms. In addition, the purpose of many algorithmic decision-making systems is to automate the personalization of users’ experiences on the basis of collected and inferred user information, which may cause or contribute to discrimination. Companies should therefore conduct human rights risk assessments related to their development and use of algorithms, as recommended by the Council of Europe in its Recommendation on the human rights impacts of algorithmic systems (2020). This indicator examines whether companies conduct robust, regular, and accountable human rights risk assessment assessments that evaluate their policies and practices relating to their development and deployment of algorithmic systems. These assessments should be part of the company’s formal, systematic due diligence activities that are aimed at ensuring that a company’s decisions and practices do not cause, contribute to, or exacerbate human rights harms. Assessments enable companies to identify possible risks of their development and deployment of algorithmic systems on users’ human rights and to take steps to mitigate possible harms if they are identified. Note that this indicator does not expect companies to publish detailed results of their human rights impact assessments, since assessments may include sensitive information. Rather, it expects that companies should disclose that they conduct HRIAs and provide information on what their HRIA process encompasses. Potential sources: Company CSR/sustainability reports Company human rights policy Global Network Initiative assessment reports"
  }, {
    "indicator": "G4e",
    "label": "G4(e) Impact assessment: Zero-rating",
    "description": "If the company engages in zero-rating, it should conduct regular, comprehensive, and credible due diligence, such as through robust human rights impact assessments, to identify how all aspects of its zero-rating policies and practices affect users’ fundamental rights to freedom of expression and information, to privacy, and to freedom from discrimination, and to mitigate any risks posed by those impacts.",
    "description_terms": ["zero-rating", "human rights impact assessments"],
    "elements": ["Does the company assess freedom of expression and information risks associated with its zero-rating programs?", "Does the company assess privacy risks associated with its zero-rating programs?", "Does the company assess discrimination risks associated with its zero-rating programs?", "Does the company conduct additional evaluation wherever the company’s risk assessments identify concerns?", "Do senior executives and/or members of the company’s board of directors review and consider the results of assessments and due diligence in their decision-making?", "Does the company conduct assessments on a regular schedule?", "Are the company’s assessments assured by an external third party?", "Is the external third party that assures the assessment accredited to a relevant and reputable human rights standard by a credible organization?"],
    "elements_tags": ["zero-rating programs", "zero-rating", "risk assessments", "senior executives", "board of directors", "assessments", "third party"],
    "guidance": "“Zero-rating” refers to programs—which can be offered by both telecommunications companies and by platforms, in partnership with telecommunications companies—that provide access to certain online services or platforms without counting against a person’s data plan. Many telecommunications providers, including RDR-ranked companies, offer such programs, either as the sole provider of the program or in partnership with social media platforms, such as Facebook’s “Free Basics.” These types of programs are a form of network prioritization that undermine net neutrality principles—and can trigger a range of other possible human rights harms, including by undermining the right to freedom of expression and information. In addition, Global Voices Advox has identified Facebook’s Free Basics as “a mechanism for collecting profitable data from users” (Global Voices, 2017), raising serious privacy concerns about the program. Zero-rating programs can also be discriminatory in the sense that they prioritize certain types of data over others, either on the basis of the protocol in question (HTTP, HTTPS, VoIP, etc.) or on the basis of the content (i.e., prioritizing one social networking site over another). This discrimination (against types of data) can in turn lead to human rights harms that affect people based on their personal characteristics, including gender, race or ethnicity, language(s) spoken, and myriad other traits. This indicator examines whether companies conduct robust, regular, and accountable impact assessments of the effects of zero-rating programs on users’ human rights. Companies that offer such programs should conduct assessments of how these programs may impact users’ rights to expression and information, privacy, and non-discrimination. These assessments should be part of the company’s formal, systematic due diligence activities that are aimed at ensuring that a company’s decisions and practices do not cause, contribute to, or exacerbate human rights harms. Assessments enable companies to identify possible risks of zero-rating programs and to take steps to mitigate possible harms if they are identified. Note that this indicator does not expect companies to publish detailed results of their human rights impact assessments, since assessments may include sensitive information. Rather, it expects that companies should disclose that they conduct HRIAs and provide information on what their HRIA process encompasses. Potential sources: Company CSR/sustainability reports Company human rights policy Global Network Initiative assessment reports"
  }, {
    "indicator": "G5",
    "label": "G5. Stakeholder engagement and accountability",
    "description": "The company should engage with a range of stakeholders on the company’s impact on freedom of expression and information, privacy, and potential risks of related human rights harms such as discrimination.",
    "description_terms": ["engage", "stakeholders", "discrimination"],
    "elements": ["Is the company a member of one or more multi-stakeholder initiatives that address the full range of ways in which users’ fundamental rights to freedom of expression and information, privacy, and non-discrimination may be affected in the course of the company’s operations?", "If the company is not a member of one or more such multi-stakeholder initiatives, is the company a member of any organizations that engages systematically and on a regular basis with non-industry and non-governmental stakeholders on freedom of expression and privacy issues?", "If the company is not a member of one of these organizations, does the company disclose that it initiates or participates in meetings with stakeholders that represent, advocate on behalf of, or are people whose rights to freedom of expression and information and to privacy are directly impacted by the company’s business?"],
    "elements_tags": ["multi-stakeholder initiatives", "stakeholders"],
    "guidance": "This indicator seeks evidence that the company engages with and is accountable to y its stakeholders—particularly with those who face human rights risks in connection with their online activities. We expect stakeholder engagement to be a core component of a company’s policy development and impact assessment process. Stakeholder engagement should be carried out across the full range of issues related to users’ freedom of expression and information, privacy, and related rights, including a company’s process for developing terms of service, privacy, and identity policies, as well as algorithmic use policies and policies governing targeted advertising, along with the enforcement practices for those policies. Stakeholder engagement and accountability mechanisms should include the full range of ways in which users’ rights may be violated: government demands, actions by other third parties via the companies’ products and services, or by the companies themselves. Companies that receive full credit on this indicator will not only engage with stakeholders but also commit to accountability processes such as independent assessments overseen by a body whose final decisions are not controlled by companies alone. Engaging with stakeholders, especially those who operate in high-risk environments, can be sensitive. A company may not feel comfortable publicly disclosing specific details about which stakeholders it consults, where or when they meet, and what they discuss. While we encourage companies to provide details about non-sensitive stakeholder engagement, we seek, at a minimum, public disclosure that a company engages with stakeholders who are or represent users whose rights to freedom of expression and privacy are at risk. One way the public knows a company participates in this type of engagement and that the engagement produces actual results is through its involvement in a multi-stakeholder initiative whose purpose is not only to create a safe space for engagement, but also to enable companies to make commitments, support them in meeting them, and hold companies accountable to them. Full and credible accountability mechanisms require multi-stakeholder governance in which companies alone do not control decision making regarding accountability processes and engagements, but rather share decision-making authority with representatives of other stakeholder constituencies. If a company receives full credit on Element 1, it will automatically receive full credit on Element 2 and Element 3. Note that because the scope of the Global Network Initiative’s work focuses on government demands, and at least half of RDR’s methodology addresses human rights threats that do not originate from governments, for the 2020 RDR Index GNI membership (without evidence of engagement and accountability on other human rights risks beyond those posed by governments) will only result in partial credit for Element 1 of this indicator. Potential sources: Company CSR/sustainability report Company annual report Company blog Company FAQ or Help Center"
  }, {
    "indicator": "G6a",
    "label": "G6(a). Remedy",
    "description": "The company should have clear and predictable grievance and remedy mechanisms to address users’ freedom of expression and privacy concerns.",
    "description_terms": ["grievance", "remedy"],
    "elements": ["Does the company clearly disclose​ it has a grievance mechanism(s) enabling users to submit complaints if they feel their freedom of expression and information rights have been adversely affected by the company’s policies or practices?", "Does the company clearly disclose​ it has a grievance mechanism(s) enabling users to submit complaints if they feel their privacy has been adversely affected by the company’s policies or practices?", "Does the company clearly disclose​ its procedures for providing remedy for freedom of expression and information-related grievances?", "Does the company clearly disclose​ its procedures for providing remedy for privacy-related grievances?", "Does the company clearly disclose​ timeframes for its grievance and remedy procedures?", "Does the company clearly disclose the number of complaints received related to freedom of expression?", "Does the company clearly disclose the number of complaints received related to privacy?", "Does the company clearly disclose​ evidence that it is providing remedy for freedom of expression grievances?", "Does the company clearly disclose​ evidence that it is providing remedy for privacy grievances?"],
    "elements_tags": ["clearly disclose​", "grievance mechanism(s)", "clearly disclose", "remedy", "grievances", "grievance"],
    "guidance": "Human rights can only be protected and respected if people have redress when they believe their rights have been violated. This indicator examines whether companies provide such remedy mechanisms and whether they have publicly disclosed processes for responding to grievances from individuals who believe that the company has violated or directly facilitated violations of their freedom of expression or privacy. We expect companies to clearly disclose a grievance mechanism enabling users to submit complaints if they feel their freedom of expression and privacy have been infringed by the company’s policies or practices. To receive full credit on Element 1, a company’s grievance mechanism does not have to explicitly state that it applies to freedom of expression and privacy related complaints. However it should be clear that this mechanism can be used to file any type of human rights-related grievance. We also expect a company’s grievance mechanism to be clearly accessible to users. In addition, the company should explain its process for providing remedy to these types of complaints, and disclose evidence of doing so. Companies should describe clear timelines for addressing each stage of the grievance and remedy processes. These standards are outlined in Principle 31 of the UN Guiding Principles on Business and Human Rights, which states that businesses should publish clear, accessible, and predictable remedy procedures. Potential sources: Company terms of service or equivalent user agreements Company content policies Company privacy policies, privacy guidelines, or privacy resource site Company CSR/sustainability report Company help center or user guide Company transparency report (for the number of complaints received) Company advertising policies"
  }, {
    "indicator": "G6b",
    "label": "G6(b). Process for content moderation appeals",
    "description": "The company should offer users clear and predictable appeals mechanisms and processes for appealing content-moderation actions.",
    "description_terms": ["appeals", "content-moderation actions"],
    "elements": ["Does the company clearly disclose​ that it offers affected users the ability to appeal content-moderation actions?", "Does the company clearly disclose​ that it notifies the users who are affected by a content-moderation action?", "Does the company clearly disclose​e a timeframe for notifying affected users when it takes a content-moderation action?", "Does the company clearly disclose​ when appeals are not permitted?", "Does the company clearly disclose​ its process for reviewing appeals?", "Does the company clearly disclose​ its timeframe for reviewing appeals?", "Does the company clearly disclose​ that such appeals are reviewed by at least one human not involved in the original content-moderation action?", "Does the company clearly disclose​ what role automation plays in reviewing appeals?", "Does the company clearly disclose​ that the affected users have an opportunity to present additional information that will be considered in the review?", "Does the company clearly disclose​ that it provides the affected users with a statement outlining the reason for its decision?", "Does the company clearly disclose​ evidence that it is addressing content moderation appeals?"],
    "elements_tags": ["clearly disclose", "affected users", "appeal", "content-moderation actions", "notifies", "affected", "content-moderation action", "notifying", "appeals"],
    "guidance": "No matter how carefully a platform crafts its terms of service, mistakes are inevitable in the demanding and subjective endeavor of content moderation. This is particularly true when content moderation is scaled rapidly through the use of automation. To respect users’ freedom of expression and information rights, companies should provide a robust and transparent appeals system that enables users to appeal decisions made by the company that directly influence users’ ability to exercise these rights. Companies should clearly disclose their process for appealing content moderation actions, including enabling affected users to immediately appeal that action. A robust appeals process should include oversight by a human reviewer and give affected users an opportunity to present additional information. Companies should also offer a clear timeframe for reviewing appeals and clearly disclose the circumstances in which appeals are not possible. To receive full credit on this indicator, companies should inform users how to submit an appeal and describe what happens once the appeal enters the pipeline. This includes notifying users of their options for appeal as soon as the company takes an initial action on their content, clarifying the role of both automation and independent human moderators in the appeals process, clearly disclosing the reason for an appeals decision and the timeframes involved, and specifying circumstances in which the appeals process is not available. Companies should also clearly demonstrate they respond to appeals by publishing data on the appeals received and the outcome of those decisions. Potential sources: Company terms of service or user agreements Company privacy policies Company sustainability report"
  }, {
    "indicator": "F1a",
    "label": "F1(a). Access to terms of service",
    "description": "The company should offer terms of service that are easy to find and easy to understand.",
    "description_terms": ["terms of service", "easy to understand"],
    "elements": ["Are the company’s terms of service easy to find?", "Are the terms of service available in the primary language(s) spoken by users in the company’s home jurisdiction?", "Are the terms of service presented in an understandable manner?"],
    "elements_tags": ["terms of service", "easy to find", "understandable manner"],
    "guidance": "A company’s terms of service outline the relationship between the user and the company. These terms contain rules about prohibited content and activities, and companies can also take action against users for violating the rules described in the terms. Given this, we expect companies to ensure that the terms are easy to access and understand. This indicator evaluates if the company’s terms are easy for users to locate. A document that is easy to find is located on the homepage of the company or service, or one or two clicks away from the homepage, or in a logical place where users can expect to find it. The use of positioning or colour schemes that make a text or link less noticeable, or hard to find on a webpage, means that the document is not easily accessible. The terms of service of an app should never be more than “two taps away” within the app (e.g. by including a “Privacy”/“Data Protection” option in the menu functionality of the app). The terms should also be available in the major language(s) of the primary operating market. In addition, we expect a company to take steps to help users understand the information presented in their documents. This includes, but is not limited to, providing summaries, tips, or guidance that explain what the terms mean, using section headers, readable font size, or other graphical features to help users understand the document, or writing the terms using readable syntax. Potential sources: Company terms of service, terms of use, terms and conditions, etc. Company acceptable use policy, community guidelines, rules, etc."
  }, {
    "indicator": "F1b",
    "label": "F1(b). Access to advertising content policies",
    "description": "The company should offer advertising content policies that are easy to find and easy to understand.",
    "description_terms": ["advertising content policies", "easy to find", "easy to understand"],
    "elements": ["Are the company’s advertising content policies easy to find?", "Are the company’s advertising content policies available in the primary language(s) spoken by users in the company’s home jurisdiction?", "Are the company’s advertising content policies presented in an understandable manner?", "(For mobile ecosystems): Does the company clearly disclose that it requires apps made available through its app store to provide users with an advertising content policy?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it requires skills made available through its skill store to provide users with an advertising content policy?"],
    "elements_tags": ["advertising content policies", "easy to find", "understandable manner", "mobile ecosystems", "clearly disclose", "app store", "advertising content policy", "personal digital assistant ecosystems", "skills", "skill store"],
    "guidance": "Companies that enable any type of advertising on their services or platforms should clearly disclose the rules for what types of ad content is prohibited—for example, ads that discriminate against individuals or groups based on personal attributes like age, religion, gender, and ethnicity. Companies should be transparent about these rules so that both users and advertisers can understand what types of ad content are not permissible and so they can be accountable for the ad content that appears on their services or platforms. Therefore, companies should make these rules easy to find (E1), easy to understand (E3), and available in the main languages of the company’s home market (E2). Companies that operate mobile ecosystems (Apple iOS, Google Android, and Samsung’s implementation of Android) and personal digital assistant ecosystems (Amazon’s Alexa, Alibaba’s AliGenie) should enable users to choose which apps or skills to download on the basis of their participation (or not) in advertising networks. Therefore, Element 4 and Element 5 ask whether the company discloses a requirement for apps or skills made available through its app store or skills store to provide users with an advertising content policy. Potential sources: Company advertising policies Company business help center Company terms of use"
  }, {
    "indicator": "F1c",
    "label": "F1(c). Access to advertising targeting policies",
    "description": "The company should offer advertising targeting policies that are easy to find and easy to understand.",
    "description_terms": ["advertising targeting policies", "easy to find", "easy to understand"],
    "elements": ["Are the company’s advertising targeting policies easy to find?", "Are the advertising targeting policies available in the primary language(s) spoken by users in the company’s home jurisdiction?", "Are the advertising targeting policies presented in an understandable manner?", "(For mobile ecosystems): Does the company clearly disclose that it requires apps made available through its app store to provide users with an advertising targeting policy?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it requires skills made available through its skill store to provide users with an advertising targeting policy?"],
    "elements_tags": ["advertising targeting policies", "easy to find", "users", "understandable manner", "mobile ecosystems", "clearly disclose", "apps", "app store", "advertising targeting policy", "personal digital assistant ecosystems", "skills", "skill store"],
    "guidance": "In addition to providing accessible ad content policies (Indicator F1b), companies should also clearly disclose their ad targeting policies. The ability for advertisers or other third parties to target users with tailored content—based on their browsing behaviors, location information, and other data and characteristics that have been inferred about them—can significantly shape (or in some cases, distort) a user’s online ecosystem. Targeting, which can include both paid and unpaid content, can amplify offline social inequities and can be overtly discriminatory. It can also result in so-called “filter bubbles” as well as amplify problematic content, including content intended to mislead or to spread falsehoods. Therefore, companies that enable advertisers and other third parties to target their users with tailored ads or content should publish targeting policies that users can easily find and understand, and that are available in the main languages of the company’s home market. Users should be able to access and understand these rules in order to make informed decisions using the information about the ad content they are receiving. For mobile ecosystems and personal digital assistant ecosystems, companies should disclose a requirement for apps or skills made available through their app stores or skill stores to provide users with an accessible advertising targeting policy. Potential sources: Company advertising policies Company business help center Company terms of use"
  }, {
    "indicator": "F1d",
    "label": "F1(d). Access to algorithmic system use policies",
    "description": "The company should offer policies related to their use of algorithms that are easy for users to find and understand.",
    "description_terms": ["algorithms", "easy for users to find", "understand"],
    "elements": ["Are the company’s algorithmic system use policies easy to find?", "Are the algorithmic system use policies available in the primary language(s) spoken by users in the company’s home jurisdiction?", "Are the algorithmic system use policies presented in an understandable manner?"],
    "elements_tags": ["algorithmic system use policies", "easy to find", "understandable manner"],
    "guidance": "The use of algorithmic systems can have adverse effects on fundamental human rights—and specifically, on the right to freedom of expression and information as well as the right to non-discrimination. In addition to clearly committing to respect and protect human rights as they develop and deploy these technologies (see Indicator G1, Element 3), companies should also publish policies that clearly describe the terms for how they use algorithmic systems across their service and platforms. Similar to having terms of service policies or user agreements that outline the terms for what types of content or activities are prohibited, companies that use algorithmic systems with the potential to cause human rights harms should publish a clear and accessible policy stating the nature and functions of these systems. As recommended by the Council of Europe’s Recommendation on the human rights impacts of algorithmic systems (2020), this policy should be easy to find, presented in plain language, and contains options for users to manage settings. Note that in this indicator, we are looking for a policy that explains terms for how the company deploys algorithmic systems across its platforms and services. We also look for companies to disclose terms that outline how they develop and test algorithmic systems, which is addressed in Indicator P1b. Potential sources: Algorithmic system use policies Guidelines for developing algorithmic systems Privacy policy or data policy Help center"
  }, {
    "indicator": "F2a",
    "label": "F2(a). Changes to terms of service",
    "description": "The company should clearly disclose that it directly notifies users when it changes its terms of service, prior to these changes coming into effect.",
    "description_terms": ["clearly disclose", "directly notifies", "terms of service"],
    "elements": ["Does the company clearly disclose that it directly notifies users about all changes to its terms of service?", "Does the company clearly disclose how it will directly notify users of changes?", "Does the company clearly disclose the timeframe within which it directly notifies users of changes prior to these changes coming into effect?", "Does the company maintain a public archive or change log?"],
    "elements_tags": ["clearly disclose", "directly notifies", "terms of service", "directly notify", "users", "public archive", "change log"],
    "guidance": "It is common for companies to change their terms of service as their business evolves. However these changes, which can include rules about prohibited content and activities, can have a significant impact on users’ freedom of expression and information rights. We therefore expect companies to commit to notifying users when they change these terms and to providing users with information that helps them understand what these changes mean. This indicator evaluates whether companies clearly disclose the method and timeframe for notifying users about changes to their terms of service. We expect companies to commit to directly notifying users of these changes prior to changes coming into effect. The method of direct notification may differ according to the type of service; we expect companies to directly notify users in a way that users are sure to access. For services that contain user accounts, direct notification may involve sending an email or an SMS. For services that do not require a user account, direct notification may involve posting a prominent notice where users access that service. This indicator also seeks evidence that a company provides publicly available records of previous terms so that people can understand how the company’s terms have evolved over time. Potential sources: Company terms of service"
  }, {
    "indicator": "F2b",
    "label": "F2(b). Changes to advertising content policies",
    "description": "The company should clearly disclose that it directly notifies users when it changes its advertising content policies, prior to these changes coming into effect.",
    "description_terms": ["clearly disclose", "directly notifies", "advertising content policies"],
    "elements": ["Does the company clearly disclose that it directly notifies users about changes to its advertising content policies?", "Does the company clearly disclose how it will directly notify users of changes?", "Does the company clearly disclose the timeframe within which it directly notifies users of changes prior to these changes coming into effect?", "Does the company maintain a public archive or change log?", "(For mobile ecosystems): Does the company clearly disclose that it requires apps made available through its app store to notify users when the apps change their advertising content policies?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it requires skills made available through its skill store to notify users when the skills change their advertising content policies?"],
    "elements_tags": ["clearly disclose", "directly notifies", "advertising content policies", "directly notify", "users", "public archive", "change log", "mobile ecosystems", "apps", "app store", "personal digital assistant ecosystems", "skills", "skill store"],
    "guidance": "It is common for companies to change their advertising content policies as their business and services evolve. However, these changes, which may include revising rules about prohibited content and activities, can affect users’ freedom of expression and information as well as their right to non-discrimination. Companies therefore should commit to notifying users when they change these terms and to providing users with information that helps them understand what these changes mean. This indicator evaluates whether companies clearly disclose the method and timeframe for notifying users about changes prior to changes coming into effect. The method of direct notification may differ according to the type of service; we expect companies to directly notify users in a way that users are sure to access. For services that contain user accounts, direct notification may involve sending an email or an SMS. For services that do not require a user account, direct notification may involve posting a prominent notice where users access that service. This indicator also seeks evidence that a company provides publicly available records of previous terms so that people can understand how the company’s terms have evolved over time. Potential sources: Advertising policies, guidelines, terms of use, etc. Company Ads or Business Help Center"
  }, {
    "indicator": "F2c",
    "label": "F2(c). Changes to advertising targeting policies",
    "description": "The company should clearly disclose that it directly notifies users when it changes its advertising targeting policies, prior to these changes coming into effect.",
    "description_terms": ["clearly disclose", "directly notifies", "users", "advertising targeting policies"],
    "elements": ["Does the company clearly disclose that it directly notifies users about changes to its advertising targeting policies?", "Does the company clearly disclose how it will directly notify users of changes?", "Does the company clearly disclose the timeframe within which it directly notifies users of changes prior to these changes coming into effect?", "Does the company maintain a public archive or change log?", "(For mobile ecosystems): Does the company clearly disclose that it requires apps made available through its app store to directly notify users when the apps change their advertising targeting policies?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it requires skills made available through its skill store to notify users when the skills change their advertising targeting policies?"],
    "elements_tags": ["clearly disclose", "directly notifies", "users", "advertising targeting policies", "directly notify", "public archive", "change log", "mobile ecosystems", "apps", "app store", "personal digital assistant ecosystems", "skills", "skill store", "notify"],
    "guidance": "It is common for companies to change their advertising targeting policies as their business and services evolve. However, these changes can affect users’ freedom of expression and information as well as their right to non-discrimination. Companies should therefore commit to notifying users when they change these terms and to providing users with information that helps them understand what these changes mean. This indicator evaluates whether companies clearly disclose the method and timeframe for notifying users about changes prior to changes coming into effect. The method of direct notification may differ according to the type of service; we expect companies to directly notify users in a way that users are sure to access. For services that contain user accounts, direct notification may involve sending an email or an SMS. For services that do not require a user account, direct notification may involve posting a prominent notice where users access that service. This indicator also seeks evidence that a company provides publicly available records of previous terms so that people can understand how the company’s terms have evolved over time. Potential sources: Advertising policies, guidelines, terms of use, etc. Company Ads or Business Help Center"
  }, {
    "indicator": "F2d",
    "label": "F2(d). Changes to algorithmic system use policies",
    "description": "The company should clearly disclose that it directly notifies users when it changes its algorithmic system use policies, prior to these changes coming into effect.",
    "description_terms": ["clearly disclose", "directly notifies", "users", "algorithmic system use policies"],
    "elements": ["Does the company clearly disclose that it directly notifies users about changes to its algorithmic system use policies?", "Does the company clearly disclose how it will directly notify users of changes?", "Does the company clearly disclose the timeframe within which it directly notifies users of changes prior to these changes coming into effect?", "Does the company maintain a public archive or change log?"],
    "elements_tags": ["clearly disclose", "directly notifies", "users", "algorithmic system use policies", "directly notify", "public archive", "change log"],
    "guidance": "When companies change their algorithm use policies, these changes can affect users’ freedom of expression and information as well as their right to non-discrimination. Companies therefore should commit to notifying users when they change these policies and to providing users with information that helps them understand what these changes mean. This standard is in line with the Council of Europe’s Recommendation on the human rights impacts of algorithmic systems (2020). This indicator evaluates whether companies clearly disclose the method and timeframe for notifying users about changes prior to changes coming into effect. The method of direct notification may differ according to the type of service; we expect companies to directly notify users in a way that users are sure to access. For services that contain user accounts, direct notification may involve sending an email or an SMS. For services that do not require a user account, direct notification may involve posting a prominent notice where users access the service. This indicator also seeks evidence that a company provides publicly available records of previous terms so that people can understand how the company’s terms have evolved over time. Potential sources: Algorithmic system use policies Guidelines for developing algorithmic systems Privacy policy or data policy Help center"
  }, {
    "indicator": "F3a",
    "label": "F3(a). Process for terms of service enforcement",
    "description": "The company should clearly disclose the circumstances under which it may restrict content or user accounts.",
    "description_terms": ["clearly disclose", "content", "user accounts"],
    "elements": ["Does the company clearly disclose what types of content or activities it does not permit?", "Does the company clearly disclose why it may restrict a user's account?", "Does the company clearly disclose information about the processes it uses to identify content or  accounts that violate the company’s rules?", "Does the company clearly disclose how it uses algorithmic systems to flag content that might violate the company’s rules?", "Does the company clearly disclose whether any government authorities receive priority consideration when flagging content to be restricted for violating the company’s rules?", "Does the company clearly disclose whether any private entities receive priority consideration when flagging content to be restricted for violating the company’s rules?", "Does the company clearly disclose its process for enforcing its rules once violations are detected"],
    "elements_tags": ["clearly disclose", "content", "user's account", "accounts", "algorithmic systems", "flagging"],
    "guidance": "It is fair to expect companies to set rules prohibiting certain content or activities—like toxic speech or malicious behavior. However, when companies develop and enforce rules about what people can do and say on the internet—or whether they can access a service at all—they must do so in a way that is transparent and accountable. We therefore expect companies to clearly disclose what these rules are and how they enforce them. This includes information about how companies learn of material or activities that violate their terms. For example, companies may rely on outside contractors to review content and/or user activity. They may also rely on community flagging mechanisms that allow users to flag other users’ content and/or activity for company review. They may also deploy algorithmic systems to detect and flag breaches, in which case, companies should explain how these systems are used and on what types of content. We expect companies to clearly disclose whether they have a policy of granting priority or expedited consideration to any government authorities and/or members of private organizations or other entities that identify their organizational affiliation when they report content or users for allegedly violating the company’s rules. For mobile ecosystems, we expect companies to disclose the types of apps they would restrict. For personal digital assistant ecosystems, we expect companies to disclose the types of skills and search results they would restrict. In this disclosure, the company should also provide examples to help users understand what these rules mean. Potential sources: Company terms of service, user agreements Company acceptable use policy, community standards, content guidelines, abusive behavior policy, or similar document that explains the rules users have to follow. Company support, help center, or FAQ"
  }, {
    "indicator": "F3b",
    "label": "F3(b). Advertising content rules and enforcement",
    "description": "The company should clearly disclose its policies governing what types of advertising content is prohibited.",
    "description_terms": ["clearly disclose"],
    "elements": ["Does the company clearly disclose what types of advertising content it does not permit?", "Does the company clearly disclose whether it requires all advertising content be clearly labelled as such?", "Does the company clearly disclose the processes and technologies it uses to identify advertising content  or accounts that violate the company’s rules?"],
    "elements_tags": ["clearly disclose", "advertising content", "requires", "accounts"],
    "guidance": "Companies should clearly disclose policies for what types of advertising content are prohibited on a platform or service, and its processes for enforcing these rules. Specifically, this indicator asks if companies clearly disclose what types of advertising content are prohibited, if the company discloses a requirement that all advertising content be clearly labeled as such, and if it discloses its processes for enforcing these rules Potential sources: Company advertiser portal, ad policy, political ad policy Company terms of service, user contract Company acceptable use policy, community standards, content guidelines Company support, help center, or FAQ"
  }, {
    "indicator": "F3c",
    "label": "F3(c). Advertising targeting rules and enforcement",
    "description": "The company should clearly disclose its policies governing what type of advertising targeting is prohibited.",
    "description_terms": ["clearly disclose", "advertising targeting"],
    "elements": ["Does the company clearly disclose whether it enables third parties to target its users with advertising content?", "Does the company clearly disclose what types of targeting parameters are not permitted?", "Does the company clearly disclose that it does not permit advertisers to target specific individuals?", "Does the company clearly disclose that algorithmically generated advertising audience categories are evaluated by human reviewers before they can be used?", "Does the company clearly disclose information about the processes and technologies it uses to identify advertising content or accounts that violate the company’s rules?"],
    "elements_tags": ["clearly disclose", "third parties", "users", "advertising content", "targeting parameters", "advertisers", "algorithmically", "advertising audience categories", "accounts"],
    "guidance": "The ability for advertisers or other third parties to target users with tailored content—based on their browsing behaviors, location information, and other data and characteristics that have been inferred about them—can significantly shape a user’s online ecosystem. Targeting, which can include both paid and unpaid content, can amplify offline social inequities and can be overtly discriminatory. It can also result in so-called “filter bubbles,” as well as spread problematic content, including content intended to mislead or to spread falsehoods. Therefore, companies that enable advertisers and other third parties to target their users with tailored ads or content should have clear policies describing their ad targeting rules. Companies should clearly disclose whether they enable third parties to target their users with tailored ads or other types of sponsored content , and clearly disclose what targeting parameters—like using certain types of audience categories, like age, location, or other user characteristics—are not permitted . Companies should also disclose their processes for identifying breaches to targeting rules . Potential sources: Company advertiser portal, ad policy, political ad policy Company acceptable use policy Company support, help center, or advertiser FAQ"
  }, {
    "indicator": "F4a",
    "label": "F4(a). Data about content restrictions to enforce terms of service",
    "description": "The company should clearly disclose and regularly publish data about the volume and nature of actions taken to restrict content that violates the company’s rules.",
    "description_terms": ["clearly disclose", "restrict content"],
    "elements": ["Does the company publish data about the total number of pieces of content restricted for violating the company’s rules?", "Does the company publish data on the number of pieces of content restricted based on which rule was violated?", "Does the company publish data on the number of pieces of content it restricted based on the format of content? (e.g. text, image, video, live video)?", "Does the company publish data on the number of pieces of content it restricted based on the method used to identify the violation?", "Does the company publish this data at least four times a year?", "Can the data be exported as a structured data file?"],
    "elements_tags": ["content restricted", "content", "restricted", "structured data"],
    "guidance": "Companies can and should set clear rules about what types of content are not permitted on their platforms or services. This indicator expects companies to publicly disclose data about the actions they take to restrict or otherwise censor content due to breaches to the company’s rules. Publishing this data is an essential first step to holding companies accountable for enforcing their own rules and for the actions they take to moderate content on their platforms and services. Companies should publish data about the aggregate number of pieces of content they restrict, remove, or—in the case of telecommunications companies—content they block or filter, as a result of terms of services violations. They should also break out this data by violation and by the method—such as a community flagger program or automation—through which the rules violation was detected. Companies should also publish this data at least four times a year, in line with the Santa Clara Principles, and in a structured data file . Potential sources: Company transparency report Company community standards enforcement report, community guidelines enforcement report, etc."
  }, {
    "indicator": "F4b",
    "label": "F4(b). Data about account restrictions to enforce terms of service",
    "description": "The company should clearly disclose and regularly publish data about the volume and nature of actions taken to restrict accounts that violate the company’s rules.",
    "description_terms": ["clearly disclose", "restrict accounts"],
    "elements": ["Does the company publish data on the total number of accounts restricted for violating the company’s own rules?", "Does the company publish data on the number of accounts restricted based on which rule was violated?", "Does the company publish data on the number of accounts restricted based on the method used to identify the violation?", "Does the company publish this data at least four times a year?", "Can the data be exported as a structured data file?"],
    "elements_tags": ["accounts restricted", "structured data"],
    "guidance": "Companies can and should set clear rules about what types of content or activities are not permitted on their platforms or services. This indicator expects companies to publicly disclose data about the actions they take to enforce these rules. Publishing this data is an essential first step to holding companies accountable for enforcing their own rules and for the actions they take to moderate content on their platforms and services. Companies should publish data about the number of accounts they restrict as a result of terms of service violations. They should also break out this data by violation and by the method—such as a community flagger program or automation—through which the rules violation was detected. Companies should also publish this data at least four times a year, in line with the Santa Clara Principles, and in a structured data file. Potential sources: Company transparency report"
  }, {
    "indicator": "F4c",
    "label": "F4(c). Data about advertising content and advertising targeting policy enforcement",
    "description": "The company should clearly disclose and regularly publish data about the volume and nature of actions taken to restrict advertising content that violates the company’s advertising content policies and advertising targeting policies.",
    "description_terms": ["clearly disclose", "restrict", "advertising content", "advertising content policies", "advertising targeting policies"],
    "elements": ["Does the company publish the number of advertisements it restricted to enforce its advertising content policies?", "Does the company publish the number of advertisements it restricted based on which advertising content rule was violated?", "Does the company publish the total number of advertisements it restricted to enforce its advertising targeting policies?", "Does the company publish the number of advertisements it restricted based on which advertising targeting rule was violated?", "Does the company publish this data at least once a year?", "Can the data be exported as a structured data?"],
    "elements_tags": ["advertisements", "restricted", "advertising content policies", "advertising content", "advertising targeting policies", "advertising targeting rule", "structured data"],
    "guidance": "Indicators F3c and F3d ask companies to clearly disclose rules for what types of ad content and ad targeting is prohibited, respectively, and to describe its processes for enforcing these rules. This indicator, F4c, asks companies to publish evidence that it is enforcing these rules. Companies should publish data on the total number of ads it removes as a result of breaches to ad content policies, and they should also break out this data by what rule was violated. Companies should also provide evidence that it is enforcing its ad targeting policies by publishing data on the number of ads removed for violating targeting rules, and by what rule was violated. Companies should also publish this data at least once a year and in a structured data file. Potential sources: Company transparency report"
  }, {
    "indicator": "F5a",
    "label": "F5(a). Process for responding to government demands to restrict content or accounts",
    "description": "The company should clearly disclose its process for responding to government demands (including judicial orders) to remove, filter, or restrict content or accounts.",
    "description_terms": ["clearly disclose", "government demands"],
    "elements": ["Does the company clearly disclose its process for responding to non-judicial government demands?", "Does the company clearly disclose its process for responding to court orders?", "Does the company clearly disclose its process for responding to government demands from foreign jurisdictions?", "Do the company’s explanations clearly disclose the legal basis under which it may comply with government demands?", "Does the company clearly disclose that it carries out due diligence on government demands before deciding how to respond?", "Does the company commit to push back on inappropriate or overbroad demands made by governments?", "Does the company provide clear guidance or examples of implementation of its process of responding to government demands?"],
    "elements_tags": ["clearly disclose", "government demands", "court orders", "demands made by governments"],
    "guidance": "Companies often receive demands from governments to remove, filter, or restrict access to content and accounts. These requests can come from government agencies, law enforcement, and courts (both domestic and foreign). We expect companies to publicly disclose their processes for responding to these types of demands. Companies should disclose the legal reasons why it would comply with a government demand, as well as disclose a clear commitment to push back on overly broad demands. Note that our definition of “government demands” includes those that come through a “non-judicial” process, such as orders from law enforcement, as well as civil cases made by private parties that come through civil courts. Takedown requests that are made via organized processes like the U.S. Digital Millennium Copyright Act or the European Right to be Forgotten ruling are defined as “private processes” and are evaluated in Indicator F5b below. Potential sources: Company transparency report Company law enforcement guidelines Company annual reports"
  }, {
    "indicator": "F5b",
    "label": "F5(b). Process for responding to private requests for content or account restriction",
    "description": "The company should clearly disclose its process for responding to requests to remove, filter, or restrict content or accounts that come through private processes.",
    "description_terms": ["clearly disclose", "requests", "content", "accounts"],
    "elements": ["Does the company clearly disclose its process for responding to requests to remove, filter, or restrict content or accounts made through private processes?", "Do the company’s explanations clearly disclose the basis under which it may comply with requests made through private processes?", "Does the company clearly disclose that it carries out due diligence on requests made through private processes before deciding how to respond?", "Does the company commit to push back on inappropriate or overbroad requests made through private processes?", "Does the company provide clear guidance or examples of implementation of its process of responding to requests made through private processes?"],
    "elements_tags": ["clearly disclose", "requests", "private processes"],
    "guidance": "In addition to demands from governments and other types of authorities, companies can receive requests to remove or restrict access to content and accounts through private processes. These types of requests can come through formal processes established by law, (e.g., requests made under the U.S. Digital Millennium Copyright Act, the European Right to be Forgotten ruling, etc.) or via self-regulatory arrangements (e.g., company agreements to block certain types of materials or images, such as via the EU’s Code of Conduct on Disinformation). Note that this indicator does not regard private requests to be requests that come through any kind of court or judicial process, which are considered under “government” requests (Indicator F5a). This indicator evaluates whether the company clearly discloses how it responds to requests to remove, filter, or restrict content or accounts that come through these types of private processes (Element 1). The company should disclose the basis for complying with these types of requests (Element 2), and whether it conducts due diligence on these requests before deciding how to respond (Element 3). We also expect companies to commit to push back on overly broad requests to remove content or accounts that come through private processes (Element 4), and to publish clear examples that illustrate how a company handles these types of requests (Element 5). Potential sources: Company transparency report Company help or support center Company blog posts Company policy on copyright or intellectual property"
  }, {
    "indicator": "F6",
    "label": "F6. Data about government demands to restrict for content and accounts",
    "description": "The company should regularly publish data about government demands (including judicial orders) to remove, filter, or restrict content and accounts.",
    "description_terms": ["government demands", "content", "accounts"],
    "elements": ["Does the company break out the number of government demands it receives by country?", "Does the company list the number of accounts affected?", "Does the company list the number of pieces of content or URLs affected?", "Does the company list the types of subject matter associated with the government demands it receives?", "Does the company list the number of government demands that come from different legal authorities?", "Does the company list the number of government demands it knowingly receives from government officials to restrict content or accounts through unofficial processes?", "Does the company list the number of government demands with which it complied?", "Does the company publish the original government demands or disclose that it provides copies to a public third-party archive?", "Does the company report this data at least once a year?"],
    "elements_tags": ["government demands", "accounts", "content", "unofficial processes", "public third-party archive"],
    "guidance": "Companies frequently receive demands from governments to remove, filter, or restrict content or accounts. We expect companies- to regularly publish data about the number and type of government demands it receives, and the number of such requests with which it complies. Companies may receive these demands through official processes, such as with a court order, or through informal channels, like through a company’s flagging system intended to allow private individuals to report content that violates the terms of service. Companies should be transparent about the nature of these requests. If a company knows that a request is coming from a government entity or court, the company should disclose it as part of its government requests reporting. Disclosing this data helps the public gain a greater understanding of the relationship between companies and governments in policing content online, and helps the public hold companies and governments accountable for their obligations to respect and protect freedom of expression rights. In some cases, the law might prevent a company from disclosing information referenced in this indicator’s elements. For example, we expect companies to publish exact numbers rather than ranges of numbers. We acknowledge that laws sometimes prevent companies from doing so, and researchers will document situations where this is the case. But a company will nonetheless lose points if it fails to meet the standards specified in all of the above elements. This represents a situation where the law causes companies to fall short of best practice, and we encourage companies to advocate for laws that enable them to fully respect users’ rights to freedom of expression and privacy. Potential sources: Company transparency report"
  }, {
    "indicator": "F7",
    "label": "F7. Data about private requests for content or account restriction",
    "description": "The company should regularly publish data about requests to remove, filter, or restrict access to content or accounts that come through private processes.",
    "description_terms": ["content", "accounts"],
    "elements": ["Does the company break out the number of requests to restrict content or accounts that it receives through private processes?", "Does the company list the number of accounts affected?", "Does the company list the number of pieces of content or URLs affected?", "Does the company list the reasons for removal associated with the requests it receives?", "Does the company clearly disclose the private processes that made requests?", "Does the company list the number of requests it complied with?", "Does the company publish the original requests or disclose that it provides copies to a public third-party archive?", "Does the company report this data at least once a year?", "Can the data be exported as a structured data file?", "Does the company clearly disclose that its reporting covers all types of requests that it receives through private processes?"],
    "elements_tags": ["content", "accounts", "clearly disclose", "private processes", "public third-party archive", "structured data"],
    "guidance": "Companies frequently receive requests to remove, filter, or restrict content or accounts through private processes, such as requests made under the U.S. Digital Millennium Copyright Act, the European Right to be Forgotten ruling, etc.) or through a self-regulatory arrangement (e.g., company agreements to block certain types of images). We expect companies to regularly publish data about the number and type of requests received through these private processes, and the number of such requests with which it complies. Potential sources: Company transparency report"
  }, {
    "indicator": "F8",
    "label": "F8. User notification about content and account restriction",
    "description": "The company should clearly disclose that it notifies users when it restricts content or accounts.",
    "description_terms": ["clearly disclose", "notifies", "users", "content", "accounts"],
    "elements": ["If the company hosts user-generated content, does the company clearly disclose that it notifies users who generated the content when it is restricted?", "Does the company clearly disclose that it notifies users who attempt to access content that has been restricted?", "In its notification, does the company clearly disclose a reason for the content restriction (legal or otherwise)?", "Does the company clearly disclose that it notifies users when it restricts their accounts?"],
    "elements_tags": ["clearly disclose", "content", "content restriction", "accounts"],
    "guidance": "Indicator F3 examines company disclosure of restrictions on what users can post or do on a service. This indicator, F8, focuses on whether a company clearly discloses that it notifies users when it takes these types of actions (whether due to terms of service enforcement or third-party restriction requests). A company’s decision to restrict or remove access to content or accounts can have a significant impact on users’ freedom of expression and access to information rights. We therefore expect a company to disclose that they notify users when they have removed content, restricted a user’s account, or otherwise restricted users’ abilities to access a service. If a company removes content that a user has posted, we expect the company to inform that user about its decision. If a different user attempts to access content that the company has restricted, we expect the company to notify that user about the content restriction. We also expect companies to specify reasons for their decisions. This disclosure should be part of companies’ explanations of their content and access restriction practices. Potential sources: Company terms of service, acceptable use policy Company community standards Company support page, help center, or FAQ Company guidelines for developers Company human rights policy"
  }, {
    "indicator": "F9",
    "label": "F9. Network management (telecommunications companies)",
    "description": "The company should clearly disclose that it does not prioritize, block, or delay certain types of traffic, applications, protocols, or content for any reason beyond assuring quality of service and reliability of the network.",
    "description_terms": ["clearly disclose", "prioritize", "applications", "protocols", "content"],
    "elements": ["Does the company clearly disclose a policy commitment to not prioritize, block, or delay certain types of traffic, applications, protocols, or content for reasons beyond assuring quality of service and reliability of the network?", "Does the company engage in practices, such as offering zero-rating programs, that prioritize network traffic for reasons beyond assuring quality of service and reliability of the network?", "If the company does engage in network prioritization practices for reasons beyond assuring quality of service and reliability of the network, does it clearly disclose its purpose for doing so?"],
    "elements_tags": ["clearly disclose", "prioritize", "applications", "protocols", "content", "zero-rating programs", "prioritization"],
    "guidance": "This indicator evaluates whether telecommunications companies clearly disclose if they engage in practices that affect the flow of content through their networks, such as throttling or traffic shaping. We expect these companies to publicly commit to avoid prioritization or degradation of content. In some cases, a company may engage in legitimate traffic shaping practices in order to ensure the flow of traffic through their networks. We expect the company to publicly disclose this and to explain their purpose for doing so. Companies may engage in paid prioritization or zero rating practices, which would not fall under legitimate network management practices. A company may have a statement on its website committing to net neutrality, for example, but also offer zero rating. Potential sources: Company network management or traffic management policies Company annual reports"
  }, {
    "indicator": "F10",
    "label": "F10. Network shutdown (telecommunications companies)",
    "description": "The company should clearly disclose the circumstances under which it may shut down or restrict access to the network or to specific protocols, services, or applications on the network.",
    "description_terms": ["clearly disclose", "shut down or restrict access to the network", "protocols", "applications"],
    "elements": ["Does the company clearly disclose the reason(s) why it may shut down service to a particular area or group of users?", "Does the company clearly disclose why it may restrict access to specific applications or protocols (e.g., VoIP, messaging) in a particular area or to a specific group of users?", "Does the company clearly disclose its process for responding to government demands to shut down a network or restrict access to a service?", "Does the company clearly disclose a commitment to push back on government demands to shut down a network or restrict access to a service?", "Does the company clearly disclose that it notifies users directly when it shuts down a network or restricts access to a service?", "Does the company clearly disclose the number of network shutdown demands it receives?", "Does the company clearly disclose the specific legal authority that makes the demands?", "Does the company clearly disclose the number of government demands with which it complied?"],
    "elements_tags": ["clearly disclose", "applications", "protocols", "government demands", "shut down a network or restrict access to a service", "shuts down a network or restricts access to a service", "network shutdown", "demands"],
    "guidance": "Network shutdowns are a growing threat to human rights. The U.N. Human Rights Council has condemned network shutdowns as a violation of international human rights law and called on governments to refrain from taking these actions. Yet governments are increasingly ordering telecommunications companies to shut down their networks, which in turn puts pressure on companies to take actions that violate their responsibility to respect human rights. We expect companies to fully disclose the circumstances under which they might take such action, to report on the demands they receive to take such actions, and to disclose commitments to push back on or mitigate the effects of government orders. Potential sources: Company terms of service Company transparency report Company law enforcement guidelines Company human rights policy"
  }, {
    "indicator": "F11",
    "label": "F11. Identity policy",
    "description": "The company should not require users to verify their identity with their government-issued identification, or other forms of identification that could be connected to their offline identity. Does the company require users to verify their identity with their government-issued identification, or with other forms of identification that could be connected to their offline identity?",
    "description_terms": ["require", "government-issued identification", "require", "government-issued identification"],
    "elements": [],
    "elements_tags": [],
    "guidance": "The ability to communicate anonymously is essential to freedom of expression both on and offline. The use of a real name online, or requiring users to provide a company with identifying information, provides a link between online activities and a specific person. This presents human rights risks to those who, for example, voice opinions that don’t align with a government’s views or who engage in activism that a government does not permit. It also presents risks for people who are persecuted for religious beliefs or sexual orientation. We therefore expect companies to disclose whether they might ask users to verify their identities using government-issued ID or other forms of identification that could be connected to their offline identity. Other forms of identification can include credit cards and registered phone numbers. We acknowledge that users may have to provide information that could be connected to their offline identity in order to access paid features of various products and services. However, users should be able to access features that don’t require payment without needing to provide information that can be tied to their offline identity. In some cases, phone numbers can be connected to a user’s offline identity, for example, in legal contexts where prepaid users are required to register with their IDs. When providing a phone number is necessary to the provision of the service (for example in the case of instant messaging apps), companies should receive full credit, unless they also require users to use their real names or submit documents that would tie their names to their offline identities. Services that require users to provide a phone number for purposes not necessary to the provision of the service will receive no credit: for example, some services may require phone numbers for two-factor authentication purposes, however, this should be optional and users should be provided with other two-factor authentication options. This indicator is applicable to digital platform companies and pre-paid mobile services (for telecommunications companies). Potential sources: Company terms of service or equivalent document Company help center Company sign up page"
  }, {
    "indicator": "F12",
    "label": "F12. Algorithmic content curation, recommendation, and/or ranking systems",
    "description": "Companies should clearly disclose how users’ online content is curated, ranked, or recommended.",
    "description_terms": ["clearly disclose", "content", "curated, ranked, or recommended"],
    "elements": ["Does the company clearly disclose whether it uses algorithmic systems to curate, recommend, and/or rank the content that users can access through its platform?", "Does the company clearly disclose how the algorithmic systems are deployed to curate, recommend, and/or rank content, including the variables that influence these systems?", "Does the company clearly disclose what options users have to control the variables that the algorithmic content curation, recommendation, and/or ranking system takes into account?", "Does the company clearly disclose whether algorithmic systems are used to automatically curate, recommend, and/or rank content by default?", "Does the company clearly disclose that users can opt in to automated content curation, recommendation, and/or ranking systems?"],
    "elements_tags": ["clearly disclose", "algorithmic systems", "curate, recommend, and/or rank", "content", "algorithmic content curation, recommendation, and/or ranking system", "curate, recommend, and/or rank content", "curation, recommendation, and/or ranking systems"],
    "guidance": "Algorithmic content curation, recommendation, and ranking systems play a critical role in shaping what types of content and information users can see and access online. In addition, systems that are optimized for user engagement can have the effect of prioritizing controversial and inflammatory content, including content that is not protected under international human rights law. Over time, reliance on algorithmic curation and recommendation systems that are optimized for engagement can alter the news and information ecosystems of entire countries or communities. These systems can be manipulated to spread disinformation and otherwise distort the information ecosystem, which can in turn fuel human rights abuses. Companies should therefore be transparent about their use of automated curation, recommendation, and ranking systems, including the variables that influence such systems. Companies should publish information about whether they use algorithmic systems to curate, recommend, and rank content. They should disclose how these systems work, what options users have to control how their information is used by these systems, whether such systems are automatically on by default, or whether users can “opt-in” to have their content automatically curated by the algorithmic system. Potential sources: Company human rights policy Company artificial intelligence policies, including AI principles, frameworks, and use guidelines Help pages describing how feed settings, home page settings, search results, recommendations, user interests, or topics are affected by algorithms"
  }, {
    "indicator": "F13",
    "label": "F13. Automated software agents (“bots”)",
    "description": "Companies should clearly disclose policies governing the use of automated software agents (“bots”) on their platforms, products and services, and how they enforce such policies.",
    "description_terms": ["clearly disclose", "automated software agents (“bots”)"],
    "elements": ["Does the company clearly disclose rules governing the use of bots on its platform?", "Does the company clearly disclose that it requires users to clearly label all content and accounts that are produced, disseminated or operated with the assistance of a bot?", "Does the company clearly disclose its process for enforcing its bot policy?", "Does the company clearly disclose data on the volume and nature of user content and account restricted for violating the company’s bot policy?"],
    "elements_tags": ["clearly disclose", "bots", "users", "content", "accounts", "bot", "bot policy", "account"],
    "guidance": "Social media platforms often allow users to create automated software agents, or “bots,” that automate various actions a user account can take, such as posting or boosting content (re-tweeting, for example). There are many innocuous or even positive uses of bots—for instance, artists use Twitter bots for the purpose of parody. There are also more problematic uses that many companies forbid or discourage, such as when political parties or their surrogates use botnets to promote certain messages or to artificially inflate a candidate’s reach in order to manipulate public discourse and outcomes. On some social media platforms, bots or coordinated networks of bots (“botnets”) can be used to harass users (“brigading”), artificially amplify certain pieces of content (mass retweeting, etc), and otherwise distort public discourse on the platform. Some experts have called for companies to require users who use bots to explicitly label them as bots, in order to help detect such distortions. Companies that allow bots therefore should have clear policies governing the use of bots on their platforms. They should disclose whether they require content and accounts that are produced, disseminated or operated with the assistance of a bot to be labelled as such. They should also clarify their process for enforcing their bot policies, including by publishing data on the volume and nature of content and accounts that are restricted for violating these rules. Potential sources: Platform policies for developers Automation or bot rules Transparency reports"
  }, {
    "indicator": "P1a",
    "label": "P1(a). Access to privacy policies",
    "description": "The company should offer privacy policies that are easy to find and easy to understand.",
    "description_terms": ["privacy policies", "easy to find", "easy to understand"],
    "elements": ["Are the company’s privacy policies easy to find?", "Are the privacy policies available in the primary language(s) spoken by users in the company’s home jurisdiction?", "Are the policies presented in an understandable manner?", "(For mobile ecosystems): Does the company disclose that it requires apps made available through its app store to provide users with a privacy policy?", "(For personal digital assistant ecosystems): Does the company disclose that it requires skills made available through its skill store to provide users with a privacy policy?"],
    "elements_tags": ["privacy policies", "easy to find", "understandable manner", "mobile ecosystems", "apps", "app store", "users", "privacy policy", "personal digital assistant ecosystems", "skills", "skill store"],
    "guidance": "Privacy policies address how companies collect, manage, use, and secure information about users as well as information provided by users. Given this, companies should ensure that users can easily locate this policy and to make an effort to help users understand what they mean. This indicator expects companies to publish privacy policies that are easy to find, are available in the primary languages spoken in the company’s home market, and easy to understand. If the company offers multiple products and services, it should be clear to what products and services the privacy policies applies. A document that is “easy to find” should be easily accessible from the company’s homepage or service website. It should be located a few clicks away from the homepage, or otherwise accessible in a logical place where users are likely to find it. The terms should also be available in the major language(s) of the home market. In addition, we expect a company to take steps to help users understand the information presented in their policies. This may include, but is not limited to, providing summaries, tips, or guidance that explain what the terms mean, using section headers, readable font size, or other graphical features to help users understand the document, or writing the terms using readable syntax. Potential sources: Company privacy policy Company data use policy"
  }, {
    "indicator": "P1b",
    "label": "P1(b). Access to algorithmic system development policies",
    "description": "The company should offer algorithmic system development policies that are easy to find and easy to understand.",
    "description_terms": ["algorithmic system development policies", "easy to find", "easy to understand"],
    "elements": ["Are the company’s algorithmic system development policies easy to find?", "Are the algorithmic system development policies available in the primary language(s) spoken by users?", "Are the algorithmic system development policies presented in an understandable manner?"],
    "elements_tags": ["algorithmic system development policies", "easy to find", "understandable manner"],
    "guidance": "Indicator guidance: The development and testing of algorithmic systems can pose significant risks to privacy, particularly when companies then use the information collected about users to develop, train, and test these systems without the data subject’s informed consent.

    Indicator guidance: Companies should clearly disclose policies describing the development and testing of algorithmic systems in a way that users can access,
    read and understand,
    so that users can make informed decisions about whether to use a company’ s products and services.Potential sources: Algorithmic system use policies Guidelines
    for developing algorithmic systems Privacy policy or data policy "
  }, {
    "indicator": "P2a",
    "label": "P2(a). Changes to privacy policies",
    "description": "The company should clearly disclose that it directly notifies users when it changes its privacy policies, prior to these changes coming into effect.",
    "description_terms": ["clearly disclose", "directly notifies", "privacy policies"],
    "elements": ["Does the company clearly disclose that it directly notifies users about all changes to its privacy policies?", "Does the company clearly disclose how it will directly notify users of changes?", "Does the company clearly disclose the timeframe within which it directly  notifies users of changes prior to these changes coming into effect?", "Does the company maintain a public archive or change log?", "(For mobile ecosystems): Does the company clearly disclose that it requires apps sold through its app store to notify users when the app changes its privacy policy?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it requires skills sold through its skill store to notify users when the skill changes its privacy policy?"],
    "elements_tags": ["clearly disclose", "directly", "notifies", "users", "privacy policies", "directly notify", "public archive", "change log", "privacy policy", "personal digital assistant ecosystems", "skills", "skill store"],
    "guidance": "Companies frequently change their privacy policies as their business evolves. However, these changes can affect a user’s privacy rights by changing what user information companies can collect, share, and store. We therefore expect companies to commit to notifying users when they change these policies and to providing users with information to help them understand what these changes mean. This indicator seeks clear disclosure by companies of their method and timeframe for notifying users about changes to privacy policies. We expect companies to commit to directly notifying users prior to changes coming into effect. The method of direct notification may differ based on the type of service. For services that require a user account, direct notification may involve sending an email or an SMS. For services that do not require a user account, direct notification should involve posting a prominent notice on the main webpage or platform where users access the service. This indicator also seeks evidence that a company provides publicly available records of previous policies so that people can understand how the company’s policies have evolved over time. Potential sources: Company privacy policy Company data use policy"
  }, {
    "indicator": "P2b",
    "label": "P2(b). Changes to algorithmic system development policies",
    "description": "The company should clearly disclose that it directly notifies users when it changes its algorithmic system development policies, prior to these changes coming into effect.",
    "description_terms": ["clearly disclose", "directly", "notifies", "users", "algorithmic system development policies"],
    "elements": ["Does the company clearly disclose that it directly notifies users about all changes to its algorithmic system development policies?", "Does the company clearly disclose how it will directly notify users of changes?", "Does the company clearly disclose the time frame within which it directly notifies users of changes prior to these changes coming into effect?", "Does the company maintain a public archive or change log?"],
    "elements_tags": ["clearly disclose", "directly notifies", "users", "algorithmic system development policies", "directly notify", "directly", "notifies"],
    "guidance": "Companies may change their algorithmic system development policies as their business evolves. However, these changes can have a significant impact on users’ right to privacy. We therefore expect companies to commit to notifying users when they change these policies and to providing users with information that helps them understand what these changes mean, as the Council of Europe recommends in its Recommendation on the human rights impacts of algorithmic systems (2020). This indicator seeks clear disclosure by companies of their method and timeframe for notifying users about changes to privacy policies. We expect companies to commit to directly notifying users prior to changes coming into effect. The method of direct notification may differ based on the type of service. For services that require a user account, direct notification may involve sending an email or an SMS. For services that do not require a user account, direct notification should involve posting a prominent notice on the main webpage or platform where users access the service. This indicator also seeks evidence that a company provides publicly available records of previous policies so that people can understand how the company’s policies have evolved over time. Potential sources: Company algorithmic use policy Privacy policy or data policy"
  }, {
    "indicator": "P3a",
    "label": "P3(a). Collection of user information",
    "description": "The company should clearly disclose what user information it collects and how.",
    "description_terms": ["clearly disclose", "user information", "collects"],
    "elements": ["Does the company clearly disclose what types of user information it collects?", "For each type of user information the company collects, does the company clearly disclose how it collects that user information?", "Does the company clearly disclose that it limits collection of user information to what is directly relevant and necessary to accomplish the purpose of its service?", "(For mobile ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third-party apps made available through its app store disclose what user information the apps collect?", "(For mobile ecosystems): Does the company clearly disclose that it evaluates whether third-party apps made available through its app store limit collection of user information to what is directly relevant and necessary to accomplish the purpose of the app?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third-party skills made available through its skill store disclose what user information the skills collect?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it evaluates whether third-party skills made available through its skill store limit collection of user information to what is directly relevant and necessary to accomplish the purpose of the skill?"],
    "elements_tags": ["clearly disclose", "user information", "collects", "limits collection", "mobile ecosystems", ")", "privacy policies", "apps", "app store", "collect", "limit collection", "personal digital assistant ecosystems", "skills", "skill store"],
    "guidance": "Companies collect a wide range of personal information from users—from personal details and account profiles to a user’s activities and location. We expect companies to clearly disclose what user information they collect and how they do so. We also expect companies to commit to the principle of data minimization and to demonstrate how this principle shapes their practices regarding user information. If companies collect multiple types of information, we expect them to provide details on how they handle each type of information. For mobile ecosystems and personal digital assistant (PDA) ecosystems, we expect the company to clearly disclose whether the privacy policies of the apps or PDA skills that are available in its mobile app store or PDA skill store specify what user information the apps or skills collect and whether those policies comply with data minimization principles. Potential sources: Company privacy policy Company webpage or section on data protection or data collection"
  }, {
    "indicator": "P3b",
    "label": "P3(b). Inference of user information",
    "description": "The company should clearly disclose what user information it infers and how.",
    "description_terms": ["clearly disclose", "user information", "infers"],
    "elements": ["Does the company clearly disclose all the types of user information it infers on the basis of collected user information?", "For each type of user information the company infers, does the company clearly disclose how it infers that user information?", "Does the company clearly disclose that it limits inference of user information to what is directly relevant and necessary to accomplish the purpose of its service?"],
    "elements_tags": ["clearly disclose", "user information", "infers", "collected user information", "inference"],
    "guidance": "Indicator guidance: In addition to collecting information about users, companies also perform big data analytics to make inferences, or predictions, about users on the basis of the collected information. These methods might be used to make inferences about user preferences or attributes (such as race, gender, sexual orientation), and opinions (including political opinions), or to predict consumer behaviors. Without sufficient transparency and user control over data inference, privacy-invasive and non-verifiable inferences cannot be predicted, understood, or refuted by users.

    Indicator guidance: In addition to disclosing the information that they collect,
    companies should disclose what information they infer and how they infer it.They should also commit to only infer information that is relevant and necessary to provide the service.For example,
    companies should not
    try to infer their users’ religion,
    sexual orientation,
    or health status(such as by assigning them to an audience category based on this characteristic) unless that information is somehow directly necessary to accomplish the purpose of their service.Potential sources: Company privacy policy,
    cookies policy Company webpage or section on data protection or data collection "
  }, {
    "indicator": "P4",
    "label": "P4. Sharing of user information",
    "description": "The company should clearly disclose what user information it shares and with whom.",
    "description_terms": ["clearly disclose", "user information", "shares"],
    "elements": ["For each type of user information the company collects, does the company clearly disclose whether it shares that user information?", "For each type of user information the company shares, does the company clearly disclose the types of third parties with which it shares that user information?", "Does the company clearly disclose that it may share user information with government(s) or legal authorities?", "For each type of user information the company shares, does the company clearly disclose the names of all third parties with which it shares user information?", "(For mobile ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third party apps made available through its app store disclose what user information the apps share?", "(For mobile ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third party apps made available through its app store disclose the types of third parties with whom they share user information?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third party skills made available through its skill store disclose what user information the skills share?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third party skills made available through its skill store disclose the types of third parties with whom they share user information?"],
    "elements_tags": ["user information", "clearly disclose", "shares", "third parties", "share", "mobile ecosystems", "privacy policies", "third party", "apps", "app store", "third part", "personal", "digital assistant ecosystems", "skills", "skill store", "personal digital assistant ecosystems"],
    "guidance": "Companies collect a wide range of personal information from users—from our personal details and account profiles to our browsing activities and location. Companies also often share this information with third parties, including advertisers, governments, and legal authorities. We expect companies to clearly disclose what user information (as RDR defines it)they share and with whom. Companies should specify if they share user information with governments and with commercial entities. For mobile ecosystems, we expect the company to clearly disclose whether the privacy policies of the apps that are available in its app store specify what user information the apps share with third parties. Companies that operate personal digital assistant (PDA) ecosystems should require that third-party skills that they make available in their skill store to clearly disclose what types of user information is shared, and the types of third parties with whom they share it. Potential sources: Company privacy policy Company policies related to sharing data, interaction with third parties"
  }, {
    "indicator": "P5",
    "label": "P5. Purpose for collecting, inferring, and sharing user information",
    "description": "The company should clearly disclose why it collects, infers, and shares user information.",
    "description_terms": ["clearly disclose", "collects", "infers", "shares", "user information"],
    "elements": ["For each type of user information the company collects, does the company clearly disclose its purpose for collection?", "For each type of user information the company infers, does the company clearly disclose its purpose for the inference?", "Does the company clearly disclose whether it combines user information from various company services and if so, why?", "For each type of user information the company shares, does the company clearly disclose its purpose for sharing?", "Does the company clearly disclose that it limits its use of user information to the purpose for which it was collected or inferred?"],
    "elements_tags": ["user information", "collects", "disclose", "collection", "infers", "clearly disclose", "inference", "shares", "collect", "inferred"],
    "guidance": "We expect companies to clearly disclose the purpose for collecting, sharing, and inferring each type of user information it collects, shares, and infers. In addition, many companies own or operate a variety of products and services, and we expect companies to clearly disclose how user information can be shared or combined across services. Companies should also publicly commit to the principle of use limitation—meaning they publicly state in their policies that they only use data for purposes for which it was specified—in line with OECD privacy guidelines, the GDPR, and other frameworks, both for the user information they collect and infer. Potential sources: Company privacy policy"
  }, {
    "indicator": "P6",
    "label": "P6. Retention of user information",
    "description": "The company should clearly disclose how long it retains user information.",
    "description_terms": ["clearly disclose", "retains", "user information"],
    "elements": ["For each type of user information the company collects, does the company clearly disclose how long it retains that user information?", "Does the company clearly disclose what de-identified user information it retains?", "Does the company clearly disclose the process for de-identifying user information?", "Does the company clearly disclose that it deletes all user information after users terminate their account?", "Does the company clearly disclose the time frame in which it will delete user information after users terminate their account?", "(For mobile ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third-partyapps made available through its app store disclose how long they retain user information?", "(For mobile ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third-party apps made available through its app store state that all user information is deleted when users terminate their accounts or delete the app?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third-party skills made available through its skill store disclose how long they retain user information?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third-party skills made available through its skill store state that all user information is deleted when users terminate their accounts or delete the skill?"],
    "elements_tags": ["user information", "clearly disclose", "retains", "de-identified", "de-identifying", "mobile ecosystems", "privacy policies", "third-party", "apps", "app store", "app", "personal digital assistant ecosystems", "skills", "skill store", "skill"],
    "guidance": "Just as we expect companies to disclose what information they collect and share about us, we also expect companies to clearly disclose for how long they retain it and the extent to which they remove identifiers from user information they store. In addition, users should also be able to understand what happens to their information when they delete their accounts. In some cases, laws or regulations may require companies to retain certain information for a given period of time. In these cases, companies should clearly disclose these regulations to users. Companies that choose to retain user information for extended periods of time should also take steps to ensure that data is not tied to a specific user. Acknowledging the ongoing debates about the efficacy of de-identification processes, and the growing sophistication around re-identification practices, we still consider de-identification a positive step that companies can take to protect the privacy of their users. In addition, if companies collect multiple types of information, we expect them to clearly disclose for how long they retain each type of information. For mobile ecosystems and personal digital assistant (PDA) ecosystems, we expect companies to disclose whether the privacy policies of the mobile apps and PDA skills that are available in their app and skill store state how long the app or skill retains user information and whether all user information is deleted if users terminate or delete the app or the skill. Potential sources: Company privacy policy Company webpage or section on data protection or data collection"
  }, {
    "indicator": "P7",
    "label": "P7. Users’ control over their own user information",
    "description": "The company should clearly disclose to users what options they have to control the company’s collection, inference, retention and use of their user information.",
    "description_terms": ["clearly disclose", "users", "options they have to control", "collection", "inference", "retention", "user information"],
    "elements": ["For each type of user information the company collects, does the company clearly disclose whether users can control the company’s collection of this user information?", "For each type of user information the company collects, does the company clearly disclose whether users can delete this user information?", "For each type of user information the company infers on the basis of collected information, does the company clearly disclose whether users can control if the company can attempt to infer this user information?", "For each type of user information the company infers on the basis of collected information, does the company clearly disclose whether users can delete this user information?", "Does the company clearly disclose that it provides users with options to control how their user information is used for targeted advertising?", "Does the company clearly disclose that targeted advertising is off by default?", "Does the company clearly disclose that it provides users with options to control how their user information is used for the development of algorithmic systems?", "Does the company clearly disclose whether it uses user information to develop algorithmic systems by default, or not?", "(For mobile ecosystems and personal digital assistant ecosystems): Does the company clearly disclose that it provides users with options to control the device’s geolocation functions?"],
    "elements_tags": ["user information", "collects", "clearly disclose", "users", "collect", "infers", "collected", "infer", "options to control", "targeted advertising", "algorithmic systems", "mobile ecosystems", "geolocation"],
    "guidance": "We expect companies to clearly disclose what options users have to control the information that companies collect, retain, and infer about them. Enabling users to control what information about them that a company collects, infers, and retains would mean giving users the ability to delete specific types of user information without requiring them to delete their entire account. We therefore expect companies to clearly disclose whether users have the option to delete specific types of user information. In addition, we expect companies to enable users to control the use of their information for the purpose of targeted advertising and algorithmic system development. Targeted advertising requires extensive collection, retention, and inference of user information, and companies should therefore clearly disclose whether users have options to control how their information is being used for these purposes. For mobile ecosystems and personal digital assistant (PDA) ecosystems, we expect companies to clearly disclose what options users have to control the collection of their location information. A user’s location changes frequently and many users carry their mobile devices nearly everywhere, making the collection of this type of information particularly sensitive. In addition, the location settings on mobile ecosystems and personal digital assistant ecosystems can influence how other products and services access their location information. For instance, mobile apps or PDA ecosystem skills may enable users to control location information. However, if the device on which those mobile apps or PDA skills run collects geolocation data by default and does not give users a way to turn this off, users may not be able to limit mobile apps’ or PDA skills’ collection of their location information. For these reasons, we expect companies to disclose that users can control how their device interacts with their location information. Potential sources: Company privacy policy Company account settings page, privacy dashboards Company help center"
  }, {
    "indicator": "P8",
    "label": "P8. Users’ access to their own user information",
    "description": "Companies should allow users to obtain all of their user information the company holds.",
    "description_terms": ["user information"],
    "elements": ["Does the company clearly disclose that users can obtain a copy of their user information?", "Does the company clearly disclose what user information users can obtain?", "Does the company clearly disclose that users can obtain their user information in a structured data format?", "Does the company clearly disclose that users can obtain all public-facing and private user information a company holds about them?", "Does the company clearly disclose that users can access the list of advertising audience categories to which the company has assigned them?", "Does the company clearly disclose that users can obtain all the information that a company has inferred about them?", "(For mobile ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third-party apps made available through its app store disclose that users can obtain all of the user information about them the app holds?", "(For personal digital assistant ecosystems): Does the company clearly disclose that it evaluates whether the privacy policies of third-party skills made available through its skill store state that all user information is deleted when users terminate their accounts or delete the skill?"],
    "elements_tags": ["clearly disclose", "user information", "users", "advertising audience categories", "infer", "mobile ecosystems", "privacy policies", "third-party", "apps", "app store", "personal digital assistant ecosystems", "skills", "skill store", "skill"],
    "guidance": "Users should be able to obtain all public-facing and internal information that companies hold about them, including the information that a company has used to make inferences, or predictions, about them. We expect companies to clearly disclose what options users have to obtain this information, what data this record contains, and what formats users can obtain it in. Companies should also enable users to access the list of advertising categories that they have been assigned to. In order to target ads, companies typically assign each user to any number of audience categories. Advertisers can then select which audience categories they want to target. Users should be able to know which audience categories the company has assigned them to, on the basis of information that the company has collected or inferred about users . For mobile ecosystems, we expect the company to disclose to users whether the apps that are available in its app store specify that users can obtain all of the user information that app holds about them. We expect companies that operate personal digital assistant (PDA) skill stores to set minimum standards that the third-party skills hosted on their platform must meet. Just as we expect companies themselves to disclose that users can obtain a record of their own user information from the company, PDA skill stores should require skills in their store to provide similar disclosure. Potential sources: Company privacy policy Company account settings Company help center Company blog posts"
  }, {
    "indicator": "P9",
    "label": "P9. Collection of user information from third parties",
    "description": "The company should clearly disclose its practices with regard to user information it collects from third-party websites or apps through technical means, as well as user information it collects through non-technical means.",
    "description_terms": ["clearly disclose", "user information", "apps", "technical means", "user information", "non-technical means"],
    "elements": ["(For digital platforms) Does the company clearly disclose what user information it collects from third-party websites through technical means?", "(For digital platforms) Does the company clearly explain how it collects user information from third parties through technical means?", "(For digital platforms) Does the company clearly disclose its purpose for collecting user information from third parties through technical means?", "(For digital platforms) Does the company clearly disclose how long it retains the user information it collects from third parties through technical means?", "(For digital platforms) Does the company clearly disclose that it respects user-generated signals to opt out of data collection?", "Does the company clearly disclose what user information it collects from third parties through non-technical means?", "Does the company clearly disclose how it collects user information from third parties through non-technical means?", "Does the company clearly disclose its purpose for collecting user information from third parties through non-technical means?", "Does the company clearly disclose how long it retains the user information it collects from third parties through non-technical means?"],
    "elements_tags": ["user information", "technical means", "third parties", "clearly disclose", "non-technical means"],
    "guidance": "We expect companies to disclose what information about users they collect from third parties, which can mean information collected from third-party websites or apps through technical means—for instance through cookies, plug-ins, or widgets, or through non-technical means, for instance through contractual agreements. Companies can also acquire user information through non-technical means, including as part of a contractual agreement, and this acquired data can become part of a “digital dossier” that companies may hold on their users, which can then form the basis for inferred and shared user information. Companies should be transparent and accountable about these practices so that users can understand if and how their activities are being tracked by companies even when they are not on a host company’s website or when the individual is not a user of a particular service or platform. Potential sources: Company privacy policy Company policy on third parties or cookies policy"
  }, {
    "indicator": "P10a",
    "label": "P10(a). Process for responding to government demands for user information",
    "description": "The company should clearly disclose its process for responding to governments demands for user information.",
    "description_terms": ["clearly disclose", "governments demands", "user information"],
    "elements": ["Does the company clearly disclose its process for responding to non-judicial government demands?", "Does the company clearly disclose its process for responding to court orders?", "Does the company clearly disclose its process for responding to government demands from foreign jurisdictions?", "Do the company’s explanations clearly disclose the legal basis under which it may comply with government demands?", "Does the company clearly disclose that it carries out due diligence on government demands before deciding how to respond?", "Does the company commit to push back on inappropriate or overbroad government demands?", "Does the company provide clear guidance or examples of implementation of its process for government demands?"],
    "elements_tags": ["clearly disclose", "non-judicial government", "demands", "court orders", "government demands"],
    "guidance": "Companies increasingly receive government demands to turn over user information. These demands can come from government agencies or courts (both domestic and foreign). We expect companies to publicly disclose their process for responding to demands from governments, along with the basis for complying with these requests. Companies should also publicly commit to pushing back on inappropriate or overbroad government demands. In some cases, the law might prevent a company from disclosing information referenced in this indicator’s elements. Researchers will document situations where this is the case, but a company will still lose points if it fails to meet standards for all elements. This represents a situation where the law causes companies to fall short of best practice, and we encourage companies to advocate for laws that enable them to fully respect users’ rights to freedom of expression and privacy. Potential sources: Company transparency report Company law enforcement guidelines Company privacy policy Company sustainability report Company blog posts"
  }, {
    "indicator": "P10b",
    "label": "P10(b). Process for responding to private requests for user information",
    "description": "The company should clearly disclose its process for responding to requests for user information that come through private processes.",
    "description_terms": ["clearly disclose", "user information", "private processes"],
    "elements": ["Does the company clearly disclose its process for responding to requests made through private processes?", "Do the company’s explanations clearly disclose the basis under which it may comply with requests made through private processes?", "Does the company clearly disclose that it carries out due diligence on requests made through private processes before deciding how to respond?", "Does the company commit to push back on inappropriate or overbroad requests made through private processes?", "Does the company provide clear guidance or examples of implementation of its process of responding to requests made through private processes?"],
    "elements_tags": ["clearly disclose", "private processes"],
    "guidance": "Companies increasingly receive private requests to turn over user information. Such requests are often informal requests for user information from a non-governmental entity that do not involve or come through any formal legal process. According to the Wikimedia Foundation—which publishes transparency reports with e data on the number of these types of such requests it receives—private requests for user information include cases in which another company sends them a letter or an email requesting “non-public information” about one of its users. This could include a user’s IP or email address. This indicator expects companies to disclose their processes for handling these types of requests. Companies should explain reasons for complying with these types of requests, and commit to push back on overly broad demands. Potential sources: Company transparency report Company law enforcement guidelines Company privacy policy Company blog posts"
  }, {
    "indicator": "P11a",
    "label": "P11(a). Data about government demands for user information",
    "description": "The company should regularly publish data about government demands for user information.",
    "description_terms": ["government", "demands", "user information"],
    "elements": ["Does the company list the number of government demands it receives by country?", "Does the company list the number of government demands it receives for stored user information and for real-time communications access?", "Does the company list the number of accounts affected?", "Does the company list whether a demand sought communications content or non-content or both?", "Does the company identify the specific legal authority or type of legal process through which law enforcement and national security demands are made?", "Does the company include government demands that come from court orders?", "Does the company list the number of government demands it complied with, broken down by category of demand?", "Does the company list what types of government demands it is prohibited by law from disclosing?", "Does the company report this data at least once per year?", "Can the data reported by the company be exported as a structured data file?"],
    "elements_tags": ["government", "demands", "real-time communications access", "content", "non-content", "court orders", "structured data"],
    "guidance": "Companies frequently receive demands from governments to hand over user information. These demands can come from government agencies or courts (both domestic and foreign). We expect companies to regularly publish data about the number and type of such demands they receive, and the number of such demands with which they comply. Companies should disclose data about requests they receive by country, including from their home and foreign governments, as well as from law enforcement and courts. We also expect company disclosure to indicate the number of accounts affected by these demands and to delineate by category the demands with which the company has complied. We recognize that companies are sometimes not legally allowed to disclose demands for user information made by governments. However, in these cases, we expect companies to report what types of government demands they are not allowed to disclose by law. Companies should also report this data once a year and should ensure the data can be exported as a structured data file. In some cases, the law might prevent a company from disclosing information referenced in this indicator. For example, we expect companies to publish exact numbers rather than ranges of numbers. We acknowledge that laws sometimes prevent companies from doing so, and researchers will document situations where this is the case. But a company will lose points if it fails to meet all elements. This represents a situation where the law causes companies to fall short of best practice, and we encourage companies to advocate for laws that enable them to fully respect users’ rights to freedom of expression and privacy. Potential sources: Company transparency report, Company law enforcement report Company sustainability report"
  }, {
    "indicator": "P11b",
    "label": "P11(b). Data about private requests for user information",
    "description": "The company should regularly publish data about requests for user information that come through private processes.",
    "description_terms": ["user information", "private processes"],
    "elements": ["Does the company list the number of requests it receives for user information that come through private processes?", "Does the company list the number of requests for user information that come through private processes with which it complied?", "Does the company report this data at least once per year?", "Can the data reported by the company be exported as a structured data file?"],
    "elements_tags": ["user information", "private processes", "private", "proc", "esses", "structured data"],
    "guidance": "Companies increasingly receive private requests to turn over user information. Such requests are often informal requests for user information from a non-governmental entity that do not involve or come through any formal legal process. According to the Wikimedia Foundation—which publishes transparency reports with data on the number of these types of requests it receives—private requests for user information includes cases in which another company sends them a letter or an email requesting “non-public information” about one of its users. This could include a user’s IP and email address. Just as companies should publish data about the government demands they receive to hand over user information, companies should also publish data about requests for user information they receive (and comply with) that come through any private processes. We expect companies to regularly publish data about the number and type of such requests they receive, and the number of such requests with which they comply. Companies should also report this data once a year and ensure the data can be exported in a structured data file. Potential sources: Company transparency report Company sustainability report Corporate social responsibility report"
  }, {
    "indicator": "P12",
    "label": "P12. User notification about third-party requests for user information",
    "description": "The company should notify users to the extent legally possible when their user information has been demanded by governments and other third parties.",
    "description_terms": ["notify", "user information", "demanded by governments", "third parties"],
    "elements": ["Does the company clearly disclose that it notifies users when government entities (including courts or other judicial bodies) demand their user information?", "Does the company clearly disclose that it notifies users when they receive requests for their user information through private processes?", "Does the company clearly disclose situations when it might not notify users, including a description of the types of government demands it is prohibited by law from disclosing to users?"],
    "elements_tags": ["clearly disclose", "government entities (including courts or other judicial bodies)", "demand", "user information", "notifies", "private processes", "notify", "government demands"],
    "guidance": "We expect companies to clearly disclose a commitment to notifying users when governments and other third parties request data about its users. We acknowledge that this notice may not be possible in legitimate cases of an ongoing investigation; however, we expect companies to specify what types of requests they are prohibited by law from disclosing. Potential sources: Company transparency report Company law enforcement guidelines Company privacy policy Company human rights policy"
  }, {
    "indicator": "P13",
    "label": "P13. Security oversight",
    "description": "The company should clearly disclose information about its institutional processes to ensure the security of its products and services.",
    "description_terms": ["clearly disclose"],
    "elements": ["Does the company clearly disclose that it has systems in place to limit and monitor employee access to user information?", "Does the company clearly disclose that it has a security team that conducts security audits on the company’s products and services?", "Does the company clearly disclose that it commissions third-party security audits on its products and services?"],
    "elements_tags": ["clearly disclose", "user information"],
    "guidance": "Because companies handle and store immense amounts of information about users, they should have clear security measures in place to ensure this information is kept secure. We expect companies to clearly disclose that they have systems in place to limit and monitor employee access to user information. We also expect the company to clearly disclose that it deploys both internal and external security teams to conduct security audits on its products and services. Potential sources: Company privacy policies Company security guide"
  }, {
    "indicator": "P14",
    "label": "P14. Addressing security vulnerabilities",
    "description": "The company should address security vulnerabilities when they are discovered.",
    "description_terms": ["security vulnerabilities"],
    "elements": ["Does the company clearly disclose that it has a mechanism through which security researchers can submit vulnerabilities they discover?", "Does the company clearly disclose the timeframe in which it will review reports of vulnerabilities?", "Does the company commit not to pursue legal action against researchers who report vulnerabilities within the terms of the company’s reporting mechanism?", "(For mobile ecosystems and personal digital assistant ecosystems) Does the company clearly disclose that software updates, security patches, add-ons, or extensions are downloaded over an encrypted channel?", "(For mobile ecosystems and telecommunications companies) Does the company clearly disclose what, if any, modifications it has made to a mobile operating system?", "(For mobile ecosystems, personal digital assistant ecosystems, and telecommunications companies) Does the company clearly disclose what, if any, effect such modifications have on the company’s ability to send security updates to users?", "(For mobile ecosystems and personal digital assistant ecosystems) Does the company clearly disclose the date through which it will continue to provide security updates for the device/OS?", "(For mobile ecosystems and personal digital assistant ecosystems) Does the company commit to provide security updates for the operating system and other critical software for a minimum of five years after release?", "(For mobile ecosystems, personal digital assistant ecosystems, and telecommunications companies) If the company uses an operating system adapted from an existing system, does the company commit to provide security patches within one month of a vulnerability being announced to the public?", "(For personal digital assistant ecosystems): Does the company clearly disclose what, if any, modifications it has made to a personal digital assistant operating system?", "(For personal digital assistant ecosystems): Does the company clearly disclose what, if any, effect such modifications have on the company’s ability to send security updates to users?"],
    "elements_tags": ["clearly disclose", "security researchers", "vulnerabilities", "researchers", "personal digital assistant ecosystems", "software updates", "patches", "encrypted", "modifications", "mobile operating system", "security updates", "device/OS", "vulnerability"],
    "guidance": "Computer code is not perfect. When companies learn of vulnerabilities that could put users and their information at risk, they should take action to mitigate those concerns. This includes ensuring that people are able to share any vulnerabilities they discover with the company. We believe it is especially important for companies to provide clear policies to users about the manner and time period in which users will receive security updates. In addition, since telecommunications providers can alter open-source mobile operating systems, we expect these companies to disclose information that may affect a user’s ability to access these critical updates. Potential sources: Company privacy policies Company security guide Company “help” forums"
  }, {
    "indicator": "P15",
    "label": "P15. Data breaches",
    "description": "The company should publicly disclose information about its processes for responding to data breaches.",
    "description_terms": ["data breaches"],
    "elements": ["Does the company clearly disclose that it will notify the relevant authorities without undue delay when a data breach occurs?", "Does the company clearly disclose its process for notifying data subjects who might be affected by a data breach?", "Does the company clearly disclose what kinds of steps it will take to address the impact of a data breach on its users?"],
    "elements_tags": ["clearly disclose", "data breach", "notifying"],
    "guidance": "Companies should have clearly disclosed processes in place for addressing data breaches, including clear policies for notifying affected users. Given that data breaches can result in significant threats to an individual’s financial or personal security, in addition to exposing private information, companies should make these processes publicly available. Individuals can then make informed decisions and consider the potential risks before signing up for a service or giving a company their information. We expect companies to have formal policies in place regarding their handling of data breaches if and when they occur, and to make this information about these policies and commitments public prior to a breach occurring. Potential sources: Company terms of service or privacy policy Company security guide"
  }, {
    "indicator": "P16",
    "label": "P16. Encryption of user communication and private content (digital platforms)",
    "description": "The company should encrypt user communication and private content so users can control who has access to it.",
    "description_terms": ["encrypt", "content", "users"],
    "elements": ["Does the company clearly disclose that the transmission of user communications is encrypted by default?", "Does the company clearly disclose that transmissions of user communications are encrypted using unique keys?", "Does the company clearly disclose that users can secure their private content using end-to-end encryption, or full-disk encryption (where applicable)?", "Does the company clearly disclose that end-to-end encryption, or full-disk encryption, is enabled by default?"],
    "elements_tags": ["clearly disclose", "encrypted", "end-to-end encryption", "full-disk encryption"],
    "guidance": "Encryption is an important tool for protecting freedom of expression and privacy. The U.N. Special Rapporteur on freedom of expression has stated unequivocally that encryption and anonymity are essential for the exercise and protection of human rights. We expect companies to clearly disclose that user communications are encrypted by default, that transmissions are protected by “perfect forward secrecy,” that users have an option to turn on end-to-end encryption, and whether it is enabled by default. For mobile ecosystems and personal digital assistant ecosystems, we expect companies to clearly disclose that they enable full-disk encryption. Potential sources: Company terms of service or privacy policy Company security guide Company help center Company sustainability reports Official company blog and/or press releases"
  }, {
    "indicator": "P17",
    "label": "P17. Account security (digital platforms)",
    "description": "The company should help users keep their accounts secure.",
    "description_terms": ["accounts"],
    "elements": ["Does the company clearly disclose that it deploys advanced authentication methods to prevent fraudulent access?", "Does the company clearly disclose that users can view their recent account activity?", "Does the company clearly disclose that it notifies users about unusual account activity and possible unauthorized access to their accounts?"],
    "elements_tags": ["clearly disclose", "notifies", "users"],
    "guidance": "Companies should help users keep their accounts secure. They should clearly disclose that they use advanced authentication techniques to prevent unauthorized access to user accounts and information. We also expect companies to provide users with tools that enable them to secure their accounts and to know when their accounts may be compromised. Potential sources: Company security center Company help pages or community support page Company account settings page Company blog"
  }, {
    "indicator": "P18",
    "label": "P18. Inform and educate users about potential risks",
    "description": "The company should publish information to help users defend themselves against cybersecurity risks.",
    "description_terms": ["cybersecurity risks"],
    "elements": "Does the company publish practical materials that educate users on how to protect themselves from cybersecurity risks relevant to their products or services?",
    "elements_tags": "cybersecurity risks",
    "guidance": "Because companies hold such vast amounts of data about users, they are often targets of malicious actors. We expect companies to help users protect themselves against such risks. This can include publishing materials on how to set up advanced account authentication or adjust privacy settings, how to avoid malware, phishing, and social engineering attacks, how to avoid or address bullying or harassment online, and what “safe browsing” means. Companies should present this guidance using clear language, ideally paired with visual materials, designed to help users understand the nature of the risks companies and users can face. These materials can take many forms including tips, tutorials, how-to guides, FAQs, or other resources presented in a way that users can easily understand. Potential sources: Company security center Company help pages or community support page Company blog"
  }]
}
